# 生產化集群管理

## Node 的生命週期管理

運營 k8s 集群, 不僅僅是集群搭建那麼簡單, 運營需要對集群中雖有節點的完整申明週期負責.

- 集群搭建
- 集群擴 / 縮容
- 集群銷毀 (很少)
- 無論是集群搭建還是擴容, 核心是 Node 的生命週期管理
  - Onboard
    - 物理資產上架
    - 操作系統安裝
    - 網絡配置
    - Kubernetes 組件安裝
    - 創建 Node 對象
  - 故障處理
    - 臨時故障 -> 重啟
    - 永久故障 -> 機器下架
  - Offboard
    - 刪除 Node 對象
    - 物理資產下架, 送修 / 報廢

## 生產化集群管理問題

- 如何設定單個集群規模
  - 社區聲明單一集群可支持 5000 節點, 在如此規模的集群中, 大規模部署應用是有諸多挑戰的. 應該更多還是更少? 如何權衡 ?
- 如何根據地域劃分集群
  - 不同地域的計算節點劃分同一集群
  - 將同一地域的節點劃分到同一集群
- 如何規劃集群的網絡
  - 企業辦公環境, 測試環境, 預生產環境和生產環境應該如何進行網絡分離
  - 不同租戶之間應如何進行網絡隔離
- 如何自動化搭建集群
  - 如何自動化搭建和升級集群, 包括自動化部署控制平面和數據平面的核心組件
  - 如何與企業的公共服務集成

## 控制平台的高可用保證

- 針對大規模的集群, 應該為控制平面組件劃分單獨節點, 減少業務容器對控制平台容器或守護進程的干擾和資源搶佔
- 控制平面所在的節點, 應確保在不同機架上, 以防止因為某些機架的交換機或電源出問題, 造成所有的控制面節點都無法工作
- 保證控制平面的每個組件有足夠的 CPU, 內存和磁盤資源, 過於嚴苛的資源限制會導致系統效率低下, 降低集群可用性
- 應盡可能地減少或消除外部依賴. 在 Kubernetes 初期版本中存在較多 Cloud Provider API 的調用, 導致在運營過程中, 當 Cloud Provider API 出現故障時, 會是的 Kubernetes 集群也無法正常工作.
- 應盡可能地將控制平面和數據平面解耦, 確保控制平面組件出現故障時, 將業務影響降到最低
- Kubernetes 還有一些核心組件, 是以普通的 Pod 形式加載運行的, 可能會被調度到任意工作節點, 與普通應用競爭資源. 這些插件是否正常運行也決定了集群的可用性

## 集群安裝方法比較

- 二進制:
  - 方法: 下載二進制文件, 並通過 systemd 來管理
  - 優點: 靈活性強
  - 缺點:
    - 複雜, 需要關心每一個組件的配置
    - 對系統服務的依賴性過多
- Kubeadm:
  - 方法:
    - Kubeadm 是搭建一個集群的命令行工具
    - 管理節點通過 kubeadm init 初始化
    - 計算節點通過 kubeadm join 加入
  - 優勢:
    - 相比二進制, 控制面板組件的安裝和配置被封裝起來
    - 管理集群的生命週期, 比如升級, 證書管理等
  - 缺點:
    - 操作系統層面的配置無自動化
    - 運行時安裝配置等複雜步驟依然是必須的
    - CNI 等插件需要手工安裝
- Kubespray:
  - 方法:
    - 通過 Ansible-playbook 完成集群搭建
  - 優點:
    - 自動化完成操作系統層面的配置
    - 利用了 kubeadm 作為集群管理工具
  - 缺點:
    - 缺少基於聲明式 API 的支持
- KOPS:
  - 方法:
    - 基於聲明式 API 的集群管理工具
  - 優勢:
    - 基於社區標準的 Cluster API 進行集群管理
    - 節點的操作系統安裝等全自動化
  - 缺點:
    - 用雲環境深度集成
    - 靈活性差

## 基於聲明式 API 管理集群

- 集群管理不僅僅包括集群搭建, 還有更多功能需要支持
  - 集群擴縮容
  - 節點健康檢查和自動修復
  - Kubernetes 升級
  - 操作系統升級

云原生場景中集群應該按照我們的期望的狀態運行, 這意味著我們應該將集群管理建立在聲明式 API 的基礎之上.

## 參與角色

- 管理集群
  - 管理 workload 集群的集群, 用來存放 Cluster API 對象的地方
- Workload 集群
  - 真正開放給用戶用來運行應用的集群, 由管理集群管理
- Infrastructure provider
  - 提供不同雲的基礎架構管理, 包括計算節點, 網絡等. 目前流行的公有雲多與 Cluster API 集成了
- Bootstrap provider
  - 證書生成
  - 控制面板組件安裝和初始化, 監控節點的創建
  - 主節點和計算節點假如集群
- Control plane
  - Kubernetes 控制平面組件

## 日常運營中的節點問題歸類

- 可自動修復的問題
  - 計算節點 Down
    - Ping 不通
    - TCP probe 失敗
    - 節點上的所有應用都不可達
- 不可自動修復的問題
  - 文件系統損壞
  - 磁盤陣列故障
  - 網盤載入問題
  - 其他硬件故障
  - Kernel 出錯, core dumps
- 其它問題
  - 軟件 Bug
  - 進程鎖死, 或者 memory / CPU 競爭問題
  - Kubernetes 組件問題
    - Kubelet / Kube-proxy / Docker / Salt

- 設定自動恢復規則
  - 大多數情況下, 重啟大法好
  - 重啟不行就重裝
  - 重裝不行就重修

## Cluster AutoScaler

### 工作機制

- 擴容
  - 由於資源不足, Pod 調度失敗, 即: 有 pod 一直處於 Pending 狀態
- 縮容
  - node 的資源利用率較低時, 持續 10 分鐘低於 50%
  - 此 node 上存在的 pod 都能被重新調度到其他 node 上運行

### Cluster AutoScaler 架構

- Autoscaler: 核心模塊, 負責整體擴縮容功能
- Estimator: 負責評估計算`擴容`節點
- Simulator: 負責模擬調度, 計算`縮容`節點
- Cloud-Provider: 與雲交互進行節點的增刪操作, 每個支持 CA 的主流廠商都實現自己的 plugin 實現動態縮放
